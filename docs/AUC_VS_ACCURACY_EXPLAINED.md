# Why AUC Increased but Accuracy Decreased

**Question**: How can AUC go up (+0.007) while accuracy goes down (-0.006)?

**Short Answer**: AUC measures ranking ability across ALL thresholds, while accuracy measures performance at ONE threshold (0.5). The model with Elo features is better at ranking but calibrated differently.

---

## ğŸ“Š The Data

### Key Finding from Analysis

```
Metric                    Without Elo     With Elo        Î”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
AUC                       0.5920          0.5988          +0.0068  âœ…
Accuracy @ 0.5 threshold  0.5873          0.5813          -0.0060  âŒ
```

But look at **different thresholds**:

```
Threshold       Without Elo     With Elo        Î”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
0.45            0.5919          0.5934          +0.0015  âœ…
0.50            0.5873          0.5813          -0.0060  âŒ
0.55            0.5422          0.5648          +0.0226  âœ…âœ…
0.60            0.5000          0.5241          +0.0241  âœ…âœ…
```

**At threshold 0.55 and 0.60, the Elo model is MUCH better!**

---

## ğŸ¯ What Happened

### Without Elo Model
- Predicts **467 home wins** (70% of games)
- Threshold 0.5 is close to optimal
- Accuracy at 0.5: **58.73%**

### With Elo Model
- Predicts only **415 home wins** (62% of games)
- More conservative - needs stronger evidence to predict home win
- Less accurate at 0.5 threshold (58.13%)
- **BUT** more accurate at higher thresholds (0.55, 0.60)

---

## ğŸ“ˆ Probability Distribution

```
Statistic                 Without Elo     With Elo        Impact
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Mean probability          0.5418          0.5368          More balanced
Std deviation             0.0764          0.0908          More spread out
Min probability           0.3358          0.3339          More confident lows
Max probability           0.7319          0.7416          More confident highs
```

**Key insight**: The Elo model has **higher standard deviation** (0.0908 vs 0.0764). This means:
- It's more confident when it predicts wins/losses
- Probabilities spread further from 0.5
- Better separation between strong and weak predictions

---

## ğŸ§  Understanding AUC vs Accuracy

### AUC (Area Under ROC Curve)
**What it measures**: Can the model RANK games correctly?

**Example**:
- Game A: Strong team vs weak team â†’ 70% home win
- Game B: Even matchup â†’ 51% home win
- Game C: Weak home vs strong away â†’ 40% home win

If actual outcomes are: A wins, B wins, C loses, the model ranked them **perfectly** (70% > 51% > 40%), even though it got B wrong at 51%.

**AUC cares about**: Is 70% > 51% > 40%? âœ…

### Accuracy at 0.5 Threshold
**What it measures**: Did we predict the correct winner using 0.5 cutoff?

**Same example**:
- Game A: 70% â†’ Predict home win â†’ Correct âœ…
- Game B: 51% â†’ Predict home win â†’ Correct âœ…
- Game C: 40% â†’ Predict away win â†’ Correct âœ…

All correct! **3/3 = 100% accuracy**

**But if model predicts B at 48% instead of 51%:**
- Game A: 70% â†’ Predict home win â†’ Correct âœ…
- Game B: 48% â†’ Predict away win â†’ **WRONG** âŒ
- Game C: 40% â†’ Predict away win â†’ Correct âœ…

Only 2/3 = 67% accuracy, but **still perfect ranking** (70% > 48% > 40%)!

---

## ğŸ” What Happened with Elo Features

### Calibration Shift

**Predictions by probability bin:**

```
Prob Range    Without Elo           With Elo
            Count    Actual Win%   Count    Actual Win%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
0.0-0.4      25      44.0%         45      44.4%     â† More games here
0.4-0.5      172     47.7%         204     49.5%     â† More games here
0.5-0.6      310     59.4%         232     58.2%     â† Fewer games here
0.6-1.0      157     65.0%         183     67.2%     â† Better calibration
```

**The Elo model**:
1. Moved **52 predictions** below 0.5 threshold (467 â†’ 415 home wins)
2. Those 52 games near the 0.5 boundary became misclassified at 0.5 threshold
3. **BUT** the model separated strong predictions (0.6+) better
4. Better separation â†’ better ranking â†’ higher AUC

---

## ğŸ’¡ Real-World Analogy

Imagine two sports analysts:

**Analyst A (Without Elo)**:
- Predicts: "70% of home teams will win"
- Very confident, calls 467/664 games for home team
- Gets 58.73% right

**Analyst B (With Elo)**:
- Predicts: "Only call home win if I'm really sure"
- More cautious, calls only 415/664 for home team
- Gets 58.13% right at 0.5 threshold (worse!)
- **BUT** when comparing any two games, Analyst B ranks them more accurately

**For betting**: Analyst B is better! You don't just need to pick winners at 50% confidence - you need to **rank opportunities** and find the best bets.

---

## ğŸ¯ Practical Implications

### For Betting
âœ… **Use AUC as primary metric** - You care about ranking (which game has best edge?)
âœ… **Adjust threshold** - Use 0.55 or 0.60 instead of 0.5
âœ… **Elo model is better** - Higher AUC = better at finding value

### Optimal Threshold
```
Without Elo: Use threshold 0.45 â†’ 59.19% accuracy
With Elo:    Use threshold 0.45 â†’ 59.34% accuracy âœ… BEST
```

At the optimal threshold (0.45), the Elo model wins: **59.34% > 59.19%**

---

## ğŸ Conclusion

**The Elo model is actually better, even though accuracy at 0.5 went down.**

Why?
1. âœ… Better AUC (0.599 vs 0.592) = better ranking
2. âœ… More confident predictions (higher std dev = better separation)
3. âœ… Better calibrated at high probabilities (0.6+)
4. âœ… At optimal threshold (0.45), accuracy is higher
5. âŒ Only worse at the arbitrary 0.5 threshold

**Recommendation**: Use the model WITH Elo features, but adjust your decision threshold based on your betting strategy.

---

## ğŸ“š Technical Explanation

**AUC = Probability that a randomly chosen positive example ranks higher than a randomly chosen negative example**

The Elo model improved this ranking ability by:
- Pulling strong predictions further from 0.5 (better confidence)
- Creating better separation between clear wins and toss-ups
- Sacrificing performance in the murky 0.45-0.55 range

This is **exactly what you want** for betting - you don't bet every game at 0.5 confidence. You want to identify the clearest opportunities, which the Elo model does better (+0.68% AUC).

**AUC is the right metric for betting. Accuracy at 0.5 is arbitrary.** ğŸ¯
